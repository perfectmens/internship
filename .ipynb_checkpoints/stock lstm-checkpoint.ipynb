{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49f4a42b-a462-43cf-b0b2-4fa3f10e44b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# --- 0. Configuration ---\n",
    "# Check if a GPU is available and set the device accordingly.\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# --- 1. Load and Inspect Data ---\n",
    "try:\n",
    "    # Load the dataset from the provided CSV file\n",
    "    df = pd.read_csv('stock_data large.csv')\n",
    "    print(\"\\nSuccessfully loaded dataset.\")\n",
    "    print(\"Dataset shape:\", df.shape)\n",
    "    print(\"First 5 rows:\")\n",
    "    print(df.head())\n",
    "except FileNotFoundError:\n",
    "    print(\"Error: 'stock_data large.csv' not found. Please ensure the file is in the correct directory.\")\n",
    "    exit()\n",
    "\n",
    "# --- 2. Data Preprocessing ---\n",
    "# Separate features (X) and the target variable (y)\n",
    "X = df.drop('Target', axis=1).values\n",
    "y = df['Target'].values\n",
    "\n",
    "# NOTE: The data is assumed to be pre-scaled. If not, a scaler like\n",
    "# MinMaxScaler from sklearn should be used here.\n",
    "X_scaled = X\n",
    "\n",
    "# --- 3. Create Time-Series Sequences ---\n",
    "def create_sequences(data, target, time_steps=60):\n",
    "    \"\"\"Creates sequences from the dataset for the LSTM model.\"\"\"\n",
    "    Xs, ys = [], []\n",
    "    for i in range(len(data) - time_steps):\n",
    "        v = data[i:(i + time_steps)]\n",
    "        Xs.append(v)\n",
    "        ys.append(target[i + time_steps])\n",
    "    return np.array(Xs), np.array(ys)\n",
    "\n",
    "TIME_STEPS = 60\n",
    "X_seq, y_seq = create_sequences(X_scaled, y, TIME_STEPS)\n",
    "\n",
    "print(f\"\\nData reshaped into sequences with {TIME_STEPS} time steps.\")\n",
    "print(\"X_seq shape:\", X_seq.shape)\n",
    "print(\"y_seq shape:\", y_seq.shape)\n",
    "\n",
    "# --- 4. Split Data and Create DataLoaders ---\n",
    "# Split data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_seq, y_seq, test_size=0.2, random_state=42, shuffle=False\n",
    ")\n",
    "\n",
    "# Convert numpy arrays to PyTorch tensors\n",
    "X_train_tensor = torch.from_numpy(X_train).float()\n",
    "y_train_tensor = torch.from_numpy(y_train).float().view(-1, 1) # Reshape for loss function\n",
    "X_test_tensor = torch.from_numpy(X_test).float()\n",
    "y_test_tensor = torch.from_numpy(y_test).float().view(-1, 1)\n",
    "\n",
    "# Create TensorDatasets and DataLoaders for batching\n",
    "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "test_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n",
    "\n",
    "BATCH_SIZE = 32\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "print(\"\\nData split and converted to PyTorch DataLoaders.\")\n",
    "print(\"Train loader size:\", len(train_loader.dataset))\n",
    "print(\"Test loader size:\", len(test_loader.dataset))\n",
    "\n",
    "\n",
    "# --- 5. Build the LSTM Model in PyTorch ---\n",
    "class LSTMModel(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, num_layers, output_dim, dropout_prob=0.2):\n",
    "        \"\"\"\n",
    "        Initializes the LSTM model layers.\n",
    "        - input_dim: Number of input features.\n",
    "        - hidden_dim: Number of features in the hidden state.\n",
    "        - num_layers: Number of stacked LSTM layers.\n",
    "        - output_dim: Number of output features.\n",
    "        \"\"\"\n",
    "        super(LSTMModel, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        # Define the LSTM layer\n",
    "        # batch_first=True makes the input/output tensors have shape (batch_size, seq_len, features)\n",
    "        self.lstm = nn.LSTM(input_dim, hidden_dim, num_layers, batch_first=True, dropout=dropout_prob)\n",
    "        \n",
    "        # Define the fully connected output layer\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "        \n",
    "        # Define the activation function\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"Defines the forward pass of the model.\"\"\"\n",
    "        # Initialize hidden state and cell state with zeros\n",
    "        # Shape: (num_layers, batch_size, hidden_dim)\n",
    "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_dim).to(x.device)\n",
    "        c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_dim).to(x.device)\n",
    "\n",
    "        # We pass the input and hidden states to the LSTM\n",
    "        out, _ = self.lstm(x, (h0, c0))\n",
    "        \n",
    "        # We take the output from the last time step\n",
    "        out = self.fc(out[:, -1, :])\n",
    "        \n",
    "        # Apply sigmoid activation for binary classification\n",
    "        out = self.sigmoid(out)\n",
    "        \n",
    "        return out\n",
    "\n",
    "# --- 6. Instantiate the Model, Loss, and Optimizer ---\n",
    "INPUT_DIM = X_train.shape[2]  # Number of features\n",
    "HIDDEN_DIM = 50\n",
    "NUM_LAYERS = 3 # Corresponds to the 3 stacked LSTM layers in the TF model\n",
    "OUTPUT_DIM = 1\n",
    "\n",
    "model = LSTMModel(INPUT_DIM, HIDDEN_DIM, NUM_LAYERS, OUTPUT_DIM).to(device)\n",
    "criterion = nn.BCELoss() # Binary Cross-Entropy Loss for binary classification\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "print(\"\\nPyTorch Model Architecture:\")\n",
    "print(model)\n",
    "\n",
    "# --- 7. Train the Model ---\n",
    "print(\"\\n--- Starting Model Training ---\")\n",
    "EPOCHS = 25\n",
    "for epoch in range(EPOCHS):\n",
    "    model.train()  # Set the model to training mode\n",
    "    for inputs, labels in train_loader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        \n",
    "        # Zero the gradients\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        # Backward pass and optimization\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "    print(f'Epoch [{epoch+1}/{EPOCHS}], Loss: {loss.item():.4f}')\n",
    "print(\"--- Model Training Complete ---\")\n",
    "\n",
    "\n",
    "# --- 8. Evaluate the Model ---\n",
    "print(\"\\n--- Evaluating Model Performance ---\")\n",
    "model.eval()  # Set the model to evaluation mode\n",
    "all_preds = []\n",
    "all_labels = []\n",
    "\n",
    "with torch.no_grad(): # Disable gradient calculation for evaluation\n",
    "    for inputs, labels in test_loader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        \n",
    "        outputs = model(inputs)\n",
    "        predicted = (outputs > 0.5).float()\n",
    "        \n",
    "        all_preds.extend(predicted.cpu().numpy())\n",
    "        all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "# Calculate accuracy\n",
    "correct_predictions = sum(p == l for p, l in zip(all_preds, all_labels))\n",
    "total_predictions = len(all_preds)\n",
    "accuracy = (correct_predictions / total_predictions) * 100\n",
    "\n",
    "print(f\"\\nFinal Model Accuracy on Test Data: {accuracy[0]:.2f}%\")\n",
    "\n",
    "\n",
    "# --- 9. Make Predictions (Example) ---\n",
    "print(\"\\n--- Example Prediction ---\")\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    # Get the first sample from the test set and add a batch dimension\n",
    "    first_sample = X_test_tensor[:1].to(device)\n",
    "    prediction_tensor = model(first_sample)\n",
    "    \n",
    "    # Convert prediction to a class\n",
    "    predicted_class = (prediction_tensor > 0.5).int().item()\n",
    "    actual_class = int(y_test_tensor[0].item())\n",
    "\n",
    "    print(f\"Prediction for the first test sample: {prediction_tensor.item():.4f}\")\n",
    "    print(f\"Predicted Class: {'1 (Up)' if predicted_class == 1 else '0 (Down)'}\")\n",
    "    print(f\"Actual Class:    {'1 (Up)' if actual_class == 1 else '0 (Down)'}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
